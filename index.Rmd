---
title: "Practical Machine Learning Course project"
author: "Marco Colbacchini"
date: "31 luglio 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Cleaning  

Read the CSV data
```{r, message=FALSE}
rm(list=ls())
set.seed(883)
library(data.table)
df1 <- read.csv("pml-training.csv",header=T, na.string=c("","NA","NULL")) 
```
Check and adjust for NA vlues and near zero variance features. 
```{r, message=FALSE}
# number of columns containing NA values 
sum(colSums(is.na(df1))!= 0)
NA_clean <- colSums(is.na(df1)) == 0
df1.clean <- df1[ , NA_clean]
# Near zero variance 
library(caret)
preproc <- nearZeroVar(df1.clean, saveMetrics=TRUE)
names(df1.clean)[preproc$nzv]
nzv_clean <- which(preproc$nzv)
df1.clean <- df1.clean[,-nzv_clean]
```
101 variables have been delated because of NA values or small variance. In addition, by looking at the remaining variables seems that the firs seven are not of interest for this analysys. 
```{r,message=FALSE}
# Unnecessary predictors 
df1.clean <- df1.clean[,-(1:7)]
```
## Subsampling 
Create a training and a validation set from the first dataframe. 
```{r,message=FALSE}
# Create training and Validation sets: 
inTrain <- createDataPartition(y=df1.clean$classe, p=0.6, list=FALSE)
training <- df1.clean[inTrain,]; validation <- df1.clean[-inTrain,]
dim(training); dim(validation)
```
## Preliminary Analysis
```{r,message=FALSE,warning = FALSE}
# Clean Memory  
rm(df1);rm(df1.clean); rm(inTrain)
gc(); gc(verbose=T)

control <- trainControl(method="cv", number=2) # my old computer do not support anything better w/o running out of memory =(
set.seed(883)
# 1. Tree  
modFit1 <- train(classe ~ ., method="rpart", data=training, trControl=control) 
library(rattle); fancyRpartPlot(modFit1$finalModel, cex=.8)
confusionMatrix(modFit1)
# 2. Boost (gbm)
modFit2 <- train(classe ~ ., method="gbm", data=training, verbose=FALSE, trControl=control) 
confusionMatrix(modFit2)
# 3. Naive Bayes
modFit3 <- train(classe ~ ., method="nb",data=training, trControl=control) 
confusionMatrix(modFit3)
# 4. Random Forest
modFit4 <- train(classe ~ .,data=training, method="rf", trControl=control, ntree=500 )
confusionMatrix(modFit4)
# 5. Support Vector Machine  
library(e1071)
f <- formula("classe ~ .")
modFit5 <- svm(f, data=training) 
```
performance on valuatiuon set. 
```{r,message=FALSE,warning = FALSE}
pred1 <- predict(modFit1,validation)
tree <- sum(pred1==validation$classe)/length(pred1)
pred2 <- predict(modFit2,validation)
boost <- sum(pred2==validation$classe)/length(pred2)
pred3 <- predict(modFit3, validation)
nb <- sum(pred3==validation$classe)/length(pred3)
pred4 <- predict(modFit4, validation)
rf <- sum(pred4==validation$classe)/length(pred4)
pred5 <- predict(modFit5, validation)
svm <- sum(pred5==validation$classe)/length(pred5)

x<-c(tree=tree,boost=boost,nb=nb,rf=rf,svm=svm)
print(x)
barplot(height=x, ylab = "Accuracy", ylim=c(0,1))
```
The random foret is by far the most accurate model, followed by boost and support vector machine. Expecially SVM may be haighly influenced by correlation in the features. 

## Pre-processing 
```{r,message=FALSE}
quantile( cor(training[,-52]), probs=c(0,0.1,0.9,1))
mat <- abs(cor(training[,-52]))
diag(mat) <- 0; sum(mat > 0.8)/2 
```
Many features are highly correlated and 15 pairs have a correlation bigger than 0.8. Principal component analysis may be a good method to adress the  redundancy in the data and it may also reduce the computation power reqired. First, the cumulative explained variancve of the principal components is plotted. 
```{r,message=FALSE}
pc <- prcomp(training[,-52],center=T,scale=T)
cumulativeV <- cumsum(pc$sdev^2/sum(pc$sdev^2)) 
plot(cumulativeV, type="l", xlab="PC",ylab="Explained Variance")
```
In order to retain 90% of the variation 20 pincipal components are required. 
```{r,message=FALSE}
preProc <- preProcess(training[,-52], method="pca", thresh = 0.9)
training <- cbind( predict(preProc,training[,-52]), classe = training$classe)  
validation <- cbind(predict(preProc,validation[,-52]) , classe = validation$classe)  
```

## Models Analysis 
Now all modells are estimated once again, using principal components instead of the original features.  
```{r, message=FALSE,warning = FALSE}
# Clean memory 
rm(modFit1); rm(modFit3) # Discard bad models 
gc(); gc(verbose=T)

set.seed(883)
# 1. Tree  
modFit6 <- train(classe ~ ., method="rpart", data=training, trControl=control) 
library(rattle); fancyRpartPlot(modFit6$finalModel, cex=.8)
confusionMatrix(modFit6)
# 2. Boost (gbm)
modFit7 <- train(classe ~ ., method="gbm", data=training, verbose=FALSE, trControl=control) 
confusionMatrix(modFit7)
# 3. Naive Bayes
modFit8 <- train(classe ~ ., method="nb",data=training, trControl=control) 
confusionMatrix(modFit8)
# 4. Random Forest
modFit9 <- train(classe ~ .,data=training, method="rf", trControl=control) 
confusionMatrix(modFit9)
# 5. Support Vector Machine  
library(e1071)
f <- formula("classe ~ .")
system.time( modFit10 <- svm(f, data=training) )
```
performance on valuation set 
```{r,message=FALSE,warning = FALSE}
pred6 <- predict(modFit6,validation)
treePC <- sum(pred6==validation$classe)/length(pred6)
pred7 <- predict(modFit7,validation)
boostPC <- sum(pred7==validation$classe)/length(pred7)
pred8 <- predict(modFit8, validation)
nbPC <- sum(pred8==validation$classe)/length(pred8)
pred9 <- predict(modFit9, validation)
rfPC <- sum(pred9==validation$classe)/length(pred9)
pred10 <- predict(modFit10, validation)
svmPC <- sum(pred10==validation$classe)/length(pred10)

x<-c(tree=treePC,boost=boostPC,nb=nbPC,rf=rfPC,svm=svmPC)
print(x)
barplot(height=x, ylab = "Accuracy", ylim=c(0,1))
```
The accuracy is smaller for all modells in comparison to the scenario which included all features. This is true even for the svm model. Apparently the major benefit of principal component analysis is more computational speed.   

## Combining Predictors
The best 3 models are combined using random forest. These are boost, random forest and svm. The validation dataset containing all variacles is used, i.e no PCA.  
```{r,message=FALSE}
df <- data.frame(pred2,pred4,pred5,classe=validation$classe)
fitComb <- train(classe ~ ., data=df, method="rf", trControl=control)
predComb <- predict(fitComb,validation)
accuracy <- sum(predComb==validation$classe)/length(predComb)
print(accuracy)
# comparison with random forest alone 
print(rf)
```
By combining the predictors there is a really small accuracy gain. However the value 
# Prediction on Testing Set 
```{r,message=FALSE}
df2 <- read.csv("pml-testing.csv",header=T, na.string=c("","NA","NULL")) 
df2.clean <- df2[ , NA_clean]
df2.clean <- df2.clean[,-nzv_clean]
df2.clean <- df2.clean[,-(1:7)]
colnames(df2.clean)[52] <- "classe"
# single model predictions
tmp1 <- predict(modFit2, df2.clean)
tmp2 <- predict(modFit4, df2.clean)
tmp3 <- predict(modFit5, df2.clean)
# Combined prediction 
tmpdf <- data.frame(pred2=tmp1,pred4=tmp2,pred5=tmp3,classe=df2.clean$classe)
answer2 <- predict(fitComb, tmpdf)
print(answer2)
```

